{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RDTH8O2PuZjS",
        "8rPuEX-qvVhK",
        "cd7iCIYbvYa6",
        "T13gdKyJvr9y",
        "nnAU3ZOIvyta",
        "m-nOQ_CTwy8i",
        "b9cL-B-2zVdz",
        "IxG3oxUlUF4J",
        "XUbRfdYP49Pb",
        "dHUEXOxAhFdy",
        "W2Mj9nCXg5yU",
        "zmeTL07vhiuB",
        "QbSDpH6ohyNC",
        "T8tWobjUC6gU",
        "LJiBrIfGTFV3",
        "OtX8pGCAoRHh",
        "yfGJhwwZr_Uo",
        "vPYVbIklz7z3",
        "giUe_ebH4_Ih",
        "li7TgyES5Du5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Train Seq2Seq + Attention model for ANY->ENG translation task**\n",
        "\n",
        "- This notebook contains everything required to train a basic seq2seq RNN model with any type of luong attention.\n",
        "- It provides flexibilty to select encoder (num of layers, uni or bi - directional) and decoder (num of layers, uni or bi - directional) architectures, attention mechanisms (\"dot\", \"concat\", \"general\").\n",
        "- It uses single to BPE based tokenizer for both languages(common vocabulary).\n",
        "- It can also be trained for bidirectional translation.\n",
        "- It provides data preprocessing utils like unicode normalizaztion, deduplication, length filtering, length-ratio filtering."
      ],
      "metadata": {
        "id": "1MtDsByUtbua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import basic stuff"
      ],
      "metadata": {
        "id": "RDTH8O2PuZjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import random\n",
        "import pathlib\n",
        "import requests\n",
        "import zipfile\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "import tokenizers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "3HhJkO63uS06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data utils"
      ],
      "metadata": {
        "id": "8rPuEX-qvVhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuration"
      ],
      "metadata": {
        "id": "cd7iCIYbvYa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp_dir = \"\"\n",
        "data_dir = \"\""
      ],
      "metadata": {
        "id": "6koFOT38vXTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preparation"
      ],
      "metadata": {
        "id": "T13gdKyJvr9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data loading"
      ],
      "metadata": {
        "id": "nnAU3ZOIvyta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path):\n",
        "  if not os.path.exists(path):\n",
        "    raise Exception(\"File not found\")\n",
        "  lines = []\n",
        "  with open(path, \"r\") as f:\n",
        "    lines = f.read().splitlines()\n",
        "  return lines"
      ],
      "metadata": {
        "id": "JKTsI11Kvv26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_data(url, path):\n",
        "  result = requests.get(url, allow_redirects=True)\n",
        "  with open(path, 'wb') as f:\n",
        "    f.write(result.content)\n",
        "  return True"
      ],
      "metadata": {
        "id": "lxjaawYCwVhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data pre-processing"
      ],
      "metadata": {
        "id": "m-nOQ_CTwy8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def null_filter(pair):\n",
        "  if len(pair[0]) == 0 or len(pair[1]) == 0:\n",
        "    return False\n",
        "  return True"
      ],
      "metadata": {
        "id": "iCGGNgruwx5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(pair):\n",
        "    return unicodedata.normalize('NFKC', pair[0]), unicodedata.normalize('NFKC', pair[1])"
      ],
      "metadata": {
        "id": "W1CVcxPnwtbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deduplication(pairs):\n",
        "  new_pairs = []\n",
        "  for pair in pairs:\n",
        "    if pair not in new_pairs:\n",
        "      new_pairs.append(pair)\n",
        "  return new_pairs"
      ],
      "metadata": {
        "id": "o90tzBn3xkvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def length_filter(pair, min_length, max_length):\n",
        "  src, tgt = pair[0], pair[1]\n",
        "  return min_length <= len(src) <= max_length and min_length <= len(tgt) <= max_length"
      ],
      "metadata": {
        "id": "KDU08INNxtfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def length_ratio_filter(pair, min_ratio, max_ratio):\n",
        "  src, tgt = pair[0], pair[1]\n",
        "  if len(src) == 0 or len(tgt) == 0:\n",
        "    return False\n",
        "  return min_ratio <= len(src) / len(tgt) <= max_ratio and min_ratio <= len(tgt) / len(src) <= max_ratio"
      ],
      "metadata": {
        "id": "wBjxx44cyrjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset formation"
      ],
      "metadata": {
        "id": "b9cL-B-2zVdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset:\n",
        "  def __init__(self, pairs, tokenizer, start_token=\"[start]\", end_token=\"[end]\", pad_token = \"[pad]\"):\n",
        "    self.pairs = pairs\n",
        "    self.tokenizer = tokenizer\n",
        "    self.start_token = start_token\n",
        "    self.end_token = end_token\n",
        "    self.pad_token = pad_token\n",
        "    self.padding_value = self.tokenizer.token_to_id(self.pad_token)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.pairs)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    src, tgt = self.pairs[idx][0], self.pairs[idx][1]\n",
        "    src_ids = self.tokenizer.encode(src).ids\n",
        "    tgt_ids = self.tokenizer.encode(f\"{self.start_token}{tgt}{self.end_token}\").ids\n",
        "    return torch.tensor(src_ids, dtype=torch.long), torch.tensor(tgt_ids, dtype=torch.long)\n",
        "\n",
        "  def collate_fn(self, batch):\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "    src_padded = torch.nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=self.padding_value)\n",
        "    tgt_padded = torch.nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=self.padding_value)\n",
        "    src_lengths = (src_padded != self.padding_value).sum(dim=1)\n",
        "    return src_padded, tgt_padded, src_lengths\n",
        "\n",
        "  def get_dataloader(self, batch_size, shuffle=True):\n",
        "    return DataLoader(self, batch_size=batch_size, shuffle=shuffle, collate_fn=self.collate_fn)"
      ],
      "metadata": {
        "id": "91khMsQQzYxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test"
      ],
      "metadata": {
        "id": "IxG3oxUlUF4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# -----------------------------\n",
        "# Create dummy tokenizer\n",
        "# -----------------------------\n",
        "sentences = [\"hello world\", \"how are you\", \"fine\", \"[pad]\", \"[start]\", \"[end]\"]\n",
        "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "trainer = WordLevelTrainer(special_tokens=[\"[pad]\", \"[start]\", \"[end]\"])\n",
        "tokenizer.train_from_iterator(sentences, trainer)\n",
        "tokenizer.token_to_id = tokenizer.model.token_to_id  # make it accessible like in your code\n",
        "\n",
        "# -----------------------------\n",
        "# Dummy translation pairs\n",
        "# -----------------------------\n",
        "pairs = [\n",
        "    (\"hello world\", \"how are you\"),\n",
        "    (\"how are you\", \"fine\"),\n",
        "    (\"hello\", \"world\"),\n",
        "    (\"\", \"\"),  # edge case: empty string\n",
        "]\n",
        "\n",
        "# Create the dataset\n",
        "dataset = TranslationDataset(pairs, tokenizer)\n",
        "\n",
        "# -----------------------------\n",
        "# Test 1: Length\n",
        "# -----------------------------\n",
        "print(\"Test 1: Dataset length\")\n",
        "assert len(dataset) == len(pairs)\n",
        "print(\"✅ Passed\")\n",
        "\n",
        "# -----------------------------\n",
        "# Test 2: __getitem__ output shapes\n",
        "# -----------------------------\n",
        "print(\"Test 2: __getitem__ returns tensors of correct shape\")\n",
        "src, tgt = dataset[0]\n",
        "assert isinstance(src, torch.Tensor) and isinstance(tgt, torch.Tensor)\n",
        "assert src.ndim == 1 and tgt.ndim == 1\n",
        "print(\"✅ Passed\")\n",
        "\n",
        "# -----------------------------\n",
        "# Test 3: Collate Function\n",
        "# -----------------------------\n",
        "print(\"Test 3: Collate function pads sequences correctly\")\n",
        "batch = [dataset[i] for i in range(3)]\n",
        "src_padded, tgt_padded, src_lengths = dataset.collate_fn(batch)\n",
        "assert src_padded.ndim == 2 and tgt_padded.ndim == 2\n",
        "assert src_padded.shape[0] == 3  # batch size\n",
        "assert src_lengths.shape == (3,)\n",
        "print(\"✅ Passed\")\n",
        "\n",
        "# -----------------------------\n",
        "# Test 4: Dataloader iterability\n",
        "# -----------------------------\n",
        "print(\"Test 4: Dataloader can be iterated\")\n",
        "loader = dataset.get_dataloader(batch_size=2)\n",
        "for src_padded, tgt_padded, src_lengths in loader:\n",
        "    assert src_padded.ndim == 2\n",
        "    assert tgt_padded.ndim == 2\n",
        "    assert src_lengths.ndim == 1\n",
        "print(\"✅ Passed\")\n",
        "\n",
        "# -----------------------------\n",
        "# Test 5: Edge Case — empty strings\n",
        "# -----------------------------\n",
        "print(\"Test 5: Edge case — empty source/target handled\")\n",
        "src, tgt = dataset[3]  # (\"\", \"\")\n",
        "assert len(src) == 0 and len(tgt) > 0  # should at least have start/end tokens\n",
        "print(\"✅ Passed\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hm7m9b_CUIVO",
        "outputId": "294d05d4-fadd-4883-d3b6-eba8f113602f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 1: Dataset length\n",
            "✅ Passed\n",
            "Test 2: __getitem__ returns tensors of correct shape\n",
            "✅ Passed\n",
            "Test 3: Collate function pads sequences correctly\n",
            "✅ Passed\n",
            "Test 4: Dataloader can be iterated\n",
            "✅ Passed\n",
            "Test 5: Edge case — empty source/target handled\n",
            "✅ Passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ],
      "metadata": {
        "id": "XUbRfdYP49Pb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokenizer(path=None, downlaod= False, save_dir=None):\n",
        "\n",
        "  if path:\n",
        "    if os.path.exists(path):\n",
        "        tokenizer = tokenizers.Tokenizer.from_file(path)\n",
        "    elif downlaod:\n",
        "        tokenizer = tokenizers.Tokenizer.from_pretrained(path)\n",
        "        if save_dir:\n",
        "          tokenizer.save(save_dir)\n",
        "        else:\n",
        "          tokenizer.save(path)\n",
        "    else:\n",
        "        raise Exception(\"File not found\")\n",
        "  else:\n",
        "    raise Exception(\"No path provided\")\n",
        "\n",
        "  if not tokenizer:\n",
        "    raise Exception(\"Tokenizer not found\")\n",
        "\n",
        "  return tokenizer"
      ],
      "metadata": {
        "id": "5R_XZwMq5Baa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_tokenizer(path, text_strs, vocab_size = 16000):\n",
        "  tokenizer = tokenizers.Tokenizer(tokenizers.models.BPE(unk_token=\"[unk]\"))\n",
        "  tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
        "  tokenizer.decoder = tokenizers.decoders.ByteLevel()\n",
        "  trainer = tokenizers.trainers.BpeTrainer(\n",
        "      vocab_size=vocab_size,\n",
        "      special_tokens=[\"[start]\", \"[end]\", \"[pad]\"],\n",
        "      show_progress=True\n",
        "    )\n",
        "  tokenizer.train_from_iterator(text_strs, trainer=trainer)\n",
        "  tokenizer.enable_padding(pad_id=tokenizer.token_to_id(\"[pad]\"), pad_token=\"[pad]\")\n",
        "  tokenizer.save(path, pretty=True)\n",
        "  return tokenizer"
      ],
      "metadata": {
        "id": "Mo-jdin49lrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Networks"
      ],
      "metadata": {
        "id": "w06elMiu_ona"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "dHUEXOxAhFdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,\n",
        "      input_size, embedding_size, hidden_size,\n",
        "      num_layers=1,\n",
        "      dropout=0.01,\n",
        "      bidirectional=False,\n",
        "      arch = \"gru\",\n",
        "      batch_first=True\n",
        "    ):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.embedding_size = embedding_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.dropout_rate = dropout\n",
        "    self.directions = 2 if bidirectional else 1\n",
        "    self.arch = arch\n",
        "    self.batch_first = batch_first\n",
        "\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    self.rnn = getattr(nn, arch.upper())(\n",
        "        embedding_size,\n",
        "        hidden_size,\n",
        "        num_layers,\n",
        "        dropout=dropout,\n",
        "        bidirectional=bidirectional,\n",
        "        batch_first=batch_first\n",
        "    )\n",
        "    self.fc = nn.Linear(self.hidden_size*self.directions, hidden_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, src_padded, src_lengths):\n",
        "    embedded = self.dropout(self.embedding(src_padded))\n",
        "    embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_lengths, batch_first=True, enforce_sorted=False)\n",
        "    # hidden: (h_n, c_n) if LSTM else (n_layer**num_directions, batch_size, hidden_dim)\n",
        "    outputs, hidden = self.rnn(embedded)\n",
        "    # output: (batch_size, max_length, hidden_dim*directions)\n",
        "    outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
        "    # output: (batch_size, max_length, hidden_dim)\n",
        "    outputs = self.fc(outputs)\n",
        "    outputs = outputs.permute(1, 0, 2)  # (max_length, batch_size, hidden_dim)\n",
        "    return outputs, hidden"
      ],
      "metadata": {
        "id": "OHM0luTu_nOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Test"
      ],
      "metadata": {
        "id": "W2Mj9nCXg5yU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "# === Define dummy input ===\n",
        "batch_size = 3\n",
        "max_seq_len = 5\n",
        "vocab_size = 10       # dummy vocab size\n",
        "pad_id = 0            # padding index\n",
        "embedding_size = 10\n",
        "hidden_size = 16\n",
        "\n",
        "# Random lengths for each sample in the batch\n",
        "src_lengths = torch.tensor([5, 3, 4])  # not sorted, which is fine due to enforce_sorted=False\n",
        "\n",
        "# Create dummy sequences (as list of tensors of different lengths)\n",
        "src_seqs = [\n",
        "    torch.tensor([1, 2, 3, 4, 5]),      # len 5\n",
        "    torch.tensor([6, 7, 8]),           # len 3\n",
        "    torch.tensor([9, 3, 4, 5])         # len 4\n",
        "]\n",
        "\n",
        "# Pad the batch\n",
        "src_padded = torch.nn.utils.rnn.pad_sequence(src_seqs, batch_first=True, padding_value=pad_id)\n",
        "print(\"src_padded:\\n\", src_padded)\n",
        "print(\"src_lengths:\", src_lengths)\n",
        "\n",
        "# === Initialize encoder ===\n",
        "encoder = Encoder(\n",
        "    input_size=vocab_size,\n",
        "    embedding_size=embedding_size,\n",
        "    hidden_size=hidden_size,\n",
        "    num_layers=3,\n",
        "    dropout=0.1,\n",
        "    bidirectional=True,\n",
        "    arch=\"lstm\",\n",
        "    batch_first=True\n",
        ")\n",
        "\n",
        "# === Forward pass ===\n",
        "outputs, hidden = encoder(src_padded, src_lengths)\n",
        "\n",
        "# === Output shapes ===\n",
        "print(\"\\nEncoder outputs shape:\", outputs.shape)  # (batch_size, max_seq_len, hidden_size)\n",
        "if isinstance(hidden, tuple):  # LSTM\n",
        "    print(\"Encoder hidden shape (h, c):\", hidden[0].shape, hidden[1].shape)\n",
        "else:  # GRU\n",
        "    print(\"Encoder hidden shape:\", hidden.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gky4gkJde3qy",
        "outputId": "1c57a19f-559b-4fb6-fec8-4f0c6938fc75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src_padded:\n",
            " tensor([[1, 2, 3, 4, 5],\n",
            "        [6, 7, 8, 0, 0],\n",
            "        [9, 3, 4, 5, 0]])\n",
            "src_lengths: tensor([5, 3, 4])\n",
            "\n",
            "Encoder outputs shape: torch.Size([3, 5, 16])\n",
            "Encoder hidden shape (h, c): torch.Size([6, 3, 16]) torch.Size([6, 3, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "qsc71U23g9IZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Attention"
      ],
      "metadata": {
        "id": "zmeTL07vhiuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LuongAttention(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(LuongAttention, self).__init__()\n",
        "        self.method = method\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        if self.method == 'general':\n",
        "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
        "\n",
        "    def dot_score(self, hidden, encoder_output):\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(encoder_output)\n",
        "        return torch.sum(hidden * energy, dim=2)\n",
        "\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "        hidden = hidden.repeat(encoder_output.size(0), 1, 1)\n",
        "        energy = self.attn(torch.cat([hidden, encoder_output], 2)).tanh()\n",
        "        return torch.sum(self.v * energy, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        if self.method == 'general':\n",
        "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'concat':\n",
        "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'dot':\n",
        "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "\n",
        "        attn_energies = attn_energies.t()\n",
        "        #  attn_weights: shape: (batch_size, 1, seq_len)\n",
        "        attn_weights = nn.functional.softmax(attn_energies, dim=1).unsqueeze(1)\n",
        "        # context: shape: (batch_size, 1, hidden_size)\n",
        "        context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1))\n",
        "        return context, attn_weights"
      ],
      "metadata": {
        "id": "GJG7QGSVOlOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Test"
      ],
      "metadata": {
        "id": "oDJkewoIhhuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_luong_attention(method='dot'):\n",
        "    batch_size = 2\n",
        "    seq_len = 4\n",
        "    hidden_size = 8\n",
        "\n",
        "    # Create dummy inputs\n",
        "    # encoder_outputs: (seq_len, batch_size, hidden_size)\n",
        "    encoder_outputs = torch.randn(seq_len, batch_size, hidden_size)\n",
        "\n",
        "    # hidden: (1, batch_size, hidden_size)\n",
        "    # Simulating the decoder hidden state at a single time step\n",
        "    hidden = torch.randn(1, batch_size, hidden_size)\n",
        "\n",
        "    # Initialize attention\n",
        "    attn = LuongAttention(method=method, hidden_size=hidden_size)\n",
        "\n",
        "    # Forward pass\n",
        "    context, attn_weights = attn(hidden, encoder_outputs)\n",
        "\n",
        "    # Print output shapes\n",
        "    print(f\"\\nTesting Luong Attention ({method.upper()})\")\n",
        "    print(\"Context shape:      \", context.shape)       # (batch_size, 1, hidden_size)\n",
        "    print(\"Attention weights:  \", attn_weights.shape)   # (batch_size, 1, seq_len)\n",
        "\n",
        "    # Basic shape assertions\n",
        "    assert context.shape == (batch_size, 1, hidden_size), \"Context shape incorrect\"\n",
        "    assert attn_weights.shape == (batch_size, 1, seq_len), \"Attention weights shape incorrect\"\n",
        "\n",
        "# Run tests for all three attention types\n",
        "for method in ['dot', 'general', 'concat']:\n",
        "    test_luong_attention(method)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6WlvQ8BhojJ",
        "outputId": "f9af15bb-718c-4eef-d5e7-fb17d818316a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing Luong Attention (DOT)\n",
            "Context shape:       torch.Size([2, 1, 8])\n",
            "Attention weights:   torch.Size([2, 1, 4])\n",
            "\n",
            "Testing Luong Attention (GENERAL)\n",
            "Context shape:       torch.Size([2, 1, 8])\n",
            "Attention weights:   torch.Size([2, 1, 4])\n",
            "\n",
            "Testing Luong Attention (CONCAT)\n",
            "Context shape:       torch.Size([2, 1, 8])\n",
            "Attention weights:   torch.Size([2, 1, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decoder"
      ],
      "metadata": {
        "id": "QbSDpH6ohyNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,\n",
        "      vocab_size, embedding_size, hidden_size,\n",
        "      num_layers=1,\n",
        "      dropout=0.01,\n",
        "      bidirectional=False,\n",
        "      batch_first=True,\n",
        "      arch = \"gru\",\n",
        "      attn = \"dot\",\n",
        "    ):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.input_size = vocab_size\n",
        "    self.embedding_size = embedding_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = vocab_size\n",
        "    self.num_layers = num_layers\n",
        "    self.dropout_rate = dropout\n",
        "    self.directions = 2 if bidirectional else 1\n",
        "    self.batch_first = batch_first\n",
        "    self.arch = arch\n",
        "    self.attn_method = attn\n",
        "\n",
        "    self.embedding = nn.Embedding(self.input_size, embedding_size)\n",
        "    self.attention = LuongAttention(attn, hidden_size)\n",
        "    self.rnn = getattr(nn, arch.upper())(\n",
        "        self.embedding_size+self.hidden_size,\n",
        "        self.hidden_size,\n",
        "        num_layers,\n",
        "        dropout=dropout,\n",
        "        bidirectional=bidirectional,\n",
        "        batch_first=batch_first\n",
        "    )\n",
        "    self.hid_attn = nn.Linear(self.num_layers * self.directions, 1)\n",
        "    self.fc = nn.Sequential(\n",
        "            nn.Linear(self.hidden_size*self.directions, self.embedding_size), nn.LeakyReLU(),\n",
        "            nn.Linear(self.embedding_size, self.output_size),\n",
        "    )\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, tgt_padded, hidden, enc_out):\n",
        "\n",
        "    if hidden is None:\n",
        "       h_0 = torch.zeros(\n",
        "          self.num_layers*self.directions,\n",
        "          tgt_padded.size(0),\n",
        "          self.hidden_size,\n",
        "          device=tgt_padded.device\n",
        "       )\n",
        "       if self.arch ==\"lstm\":\n",
        "          c_0 = torch.zeros_like(h_0)\n",
        "          hidden = (h_0, c_0)\n",
        "       else:\n",
        "          hidden = h_0\n",
        "\n",
        "    if self.arch == \"lstm\" and isinstance(hidden,tuple):\n",
        "      hidden_ = hidden[0]\n",
        "    else:\n",
        "      hidden_ = hidden\n",
        "\n",
        "    # [L*D, B, H]\n",
        "    hidden_ = hidden_.permute(1, 2, 0)           # [B, H, L*D]\n",
        "    attn_inp = self.hid_attn(hidden_)            # [B, H, 1]\n",
        "    attn_inp = attn_inp.permute(2, 0, 1)         # [1, B, H]\n",
        "\n",
        "    # print(f\"Eo: {enc_out.shape}, Attin: {attn_inp.shape}\")\n",
        "    context, attn_weights = self.attention(attn_inp, enc_out)\n",
        "    embedded = self.dropout(self.embedding(tgt_padded))\n",
        "    context = context.repeat(1, embedded.size(1), 1)\n",
        "    rnn_input = torch.cat([embedded, context], dim=2)\n",
        "    # hidden: (h_n, c_n) if LSTM else (n_layer**num_directions, batch_size, hidden_dim)\n",
        "    outputs, hidden = self.rnn(rnn_input, hidden)\n",
        "    output = self.fc(outputs)\n",
        "    return output, hidden"
      ],
      "metadata": {
        "id": "gUuSx0SP_h78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Test"
      ],
      "metadata": {
        "id": "T8tWobjUC6gU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------------------------------\n",
        "# 1. Load Tokenizer\n",
        "#---------------------------------------\n",
        "\n",
        "tokenizer = get_tokenizer(path=\"/content/en_tokenizer.json\")\n",
        "\n",
        "# ---------------------------------------\n",
        "# 2. Create dummy translation pairs\n",
        "# ---------------------------------------\n",
        "pairs = [\n",
        "    (\"hello world\", \"how are you\"),\n",
        "    (\"i am fine\", \"this is good\"),\n",
        "    (\"nice to meet you\", \"hello world\")\n",
        "]\n",
        "\n",
        "# ---------------------------------------\n",
        "# 3. Use your TranslationDataset\n",
        "# ---------------------------------------\n",
        "dataset = TranslationDataset(pairs, tokenizer)\n",
        "dataloader = dataset.get_dataloader(batch_size=2)\n",
        "\n",
        "# ---------------------------------------\n",
        "# 4. Get one batch from the dataloader\n",
        "# ---------------------------------------\n",
        "src_padded, tgt_padded, src_lengths = next(iter(dataloader))\n",
        "print(\"Source padded:\", src_padded)\n",
        "print(\"Target padded:\", tgt_padded)\n",
        "print(\"Source lengths:\", src_lengths)\n",
        "\n",
        "# ---------------------------------------\n",
        "# 5. Create a dummy encoder output\n",
        "# ---------------------------------------\n",
        "batch_size, seq_len = src_padded.size()\n",
        "hidden_size = 32\n",
        "embedding_size = 16\n",
        "output_size = len(tokenizer.get_vocab())\n",
        "\n",
        "enc_out = torch.randn(seq_len, batch_size, hidden_size)\n",
        "\n",
        "# ---------------------------------------\n",
        "# 6. Run your Decoder\n",
        "# ---------------------------------------\n",
        "decoder = Decoder(\n",
        "    input_size=output_size,\n",
        "    embedding_size=embedding_size,\n",
        "    hidden_size=hidden_size,\n",
        "    output_size=output_size,\n",
        "    num_layers=1,\n",
        "    dropout=0.1,\n",
        "    bidirectional=False,\n",
        "    batch_first=True,\n",
        "    arch=\"lstm\",\n",
        "    attn=\"dot\"\n",
        ")\n",
        "\n",
        "output, hidden = decoder(tgt_padded, None, enc_out)\n",
        "\n",
        "print(\"Decoder output shape:\", output.shape)\n",
        "print(\"Decoder hidden shape:\", hidden[0].shape if isinstance(hidden, tuple) else hidden.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9-px_ZCC7o0",
        "outputId": "a846acbb-17ca-4f80-8a8e-40a797817a75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source padded: tensor([[3548,  987,    2,    2],\n",
            "        [ 854,   80,  605,   81]])\n",
            "Target padded: tensor([[   0,  219,  157,   81,    1],\n",
            "        [   0, 3548,  987,    1,    2]])\n",
            "Source lengths: tensor([2, 4])\n",
            "Decoder output shape: torch.Size([2, 5, 8000])\n",
            "Decoder hidden shape: torch.Size([1, 2, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seq2Seq"
      ],
      "metadata": {
        "id": "LJiBrIfGTFV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self,\n",
        "                 encoder, decoder,\n",
        "                 vocab_size, hidden_sz, enc_embed_sz = 512, dec_embed_sz = 512,\n",
        "                 enc_num_layers=1, dec_num_layers=1, enc_bidir = True, dec_bidir = False,\n",
        "                 enc_dropout=0.0, dec_dropout=0.0,\n",
        "                 arch = \"gru\",  attn=\"dot\", device=\"cpu\"\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.batch_first=True\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_sz = hidden_sz\n",
        "        self.arch = arch\n",
        "        self.attn = attn\n",
        "        self.enc_dirs = 2 if enc_bidir else 1\n",
        "        self.dec_dirs = 2 if dec_bidir else 1\n",
        "        self.enc_num_layers = enc_num_layers\n",
        "        self.dec_num_layers = dec_num_layers\n",
        "        # self.enc_embed_sz = enc_embed_sz\n",
        "        # self.dec_embed_sz = dec_embed_sz\n",
        "        self.device = device\n",
        "        self.encoder = encoder(\n",
        "            vocab_size,\n",
        "            enc_embed_sz,\n",
        "            hidden_sz,\n",
        "            num_layers=enc_num_layers,\n",
        "            bidirectional=enc_bidir,\n",
        "            batch_first = self.batch_first,\n",
        "            arch = arch,\n",
        "            dropout=enc_dropout,\n",
        "            )\n",
        "\n",
        "        self.decoder = decoder(\n",
        "            vocab_size,\n",
        "            dec_embed_sz,\n",
        "            hidden_sz,\n",
        "            num_layers=dec_num_layers,\n",
        "            bidirectional=dec_bidir,\n",
        "            batch_first = self.batch_first,\n",
        "            arch = arch,\n",
        "            dropout=dec_dropout,\n",
        "            attn = attn\n",
        "        )\n",
        "        # [batch_sz, hidden_sz,enc_num_layers*enc_bidir] -> [batch_sz, hidden_sz,dec_num_layers*dec_dirs]\n",
        "        self.enc_dec = nn.Linear(enc_num_layers*self.enc_dirs, dec_num_layers*self.dec_dirs)\n",
        "        self.encoder.to(device)\n",
        "        self.decoder.to(device)\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, input_seq, input_len, target_seq = None, tfr=0.5):\n",
        "        \"\"\"Given the partial target sequence, predict the next token\"\"\"\n",
        "\n",
        "        # input seq = [batch_size, seq_len]\n",
        "        # target seq = [batch_size, seq_len]\n",
        "        batch_size, target_len = target_seq.shape\n",
        "        # device = target_seq.device\n",
        "        # storing output logits\n",
        "        outputs = []\n",
        "        # encoder forward pass\n",
        "        _enc_out, enc_hidden = self.encoder(input_seq, input_len)\n",
        "        if self.arch == \"lstm\" and isinstance(enc_hidden, tuple):\n",
        "            enc_hid_0 = enc_hidden[0] # [enc_num_layers*enc_dirs, batch_sz, hidden_sz]\n",
        "            enc_hid_1 = enc_hidden[1] # [enc_num_layers*enc_dirs, batch_sz, hidden_sz]\n",
        "            enc_hid_0, enc_hid_1 = enc_hid_0.permute(1,2,0), enc_hid_1.permute(1,2,0)  # [batch_sz, hidden_sz,enc_num_layers*enc_bidir]\n",
        "            enc_hid_0, enc_hid_1 = self.enc_dec(enc_hid_0) , self.enc_dec(enc_hid_1)\n",
        "            hidden = (enc_hid_0.permute(2,0,1), enc_hid_1.permute(2,0,1)) # [dec_num_layers*dec_dirs, batch_sz, hidden_sz]\n",
        "        else:\n",
        "            enc_hidden = enc_hidden.permute(1,2,0) # [batch_sz, hidden_sz,enc_num_layers*enc_bidir]\n",
        "            hidden = self.enc_dec(enc_hidden)\n",
        "            hidden = hidden.permute(2, 0, 1) # [dec_num_layers*dec_dirs, batch_sz, hidden_sz]\n",
        "\n",
        "        # decoder forward pass\n",
        "        dec_in = target_seq[:, :1].to(self.device)\n",
        "        # decoder forward pass\n",
        "        for t in range(target_len-1):\n",
        "            # last target token and hidden states -> next token\n",
        "            pred, hidden = self.decoder(dec_in, hidden, _enc_out)\n",
        "            # store the prediction\n",
        "            pred = pred[:, -1:, :] # [batch, 1, vocab]\n",
        "            outputs.append(pred)\n",
        "\n",
        "            use_teacher = torch.rand(1).item() < tfr\n",
        "            if use_teacher:\n",
        "                # use the target token as the next input\n",
        "                dec_in = target_seq[:, t+1:t+2]\n",
        "            else:\n",
        "                # use the predicted token as the next input\n",
        "                dec_in = pred.argmax(dim=2)\n",
        "\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "        return outputs\n",
        "\n",
        "    def generate(self, input_seq, tokenizer, max_len=50):\n",
        "        \"\"\"\n",
        "        Generate decoded sequences from input_seq.\n",
        "\n",
        "        Args:\n",
        "            input_seq (Tensor): [batch_size, seq_len] input tokens\n",
        "            tokenizer: must have tokenizer.pad_token_id, tokenizer.eos_token_id, tokenizer.decode()\n",
        "            max_len (int): max generation length\n",
        "\n",
        "        Returns:\n",
        "            List[str]: list of decoded strings (one per input)\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        pad_idx = tokenizer.token_to_id(\"[pad]\")\n",
        "        sos_idx = tokenizer.token_to_id(\"[start]\")\n",
        "        eos_idx = tokenizer.token_to_id(\"[end]\")\n",
        "\n",
        "        input_ids = tokenizer.encode(input_seq).ids\n",
        "        input_tensor = torch.tensor(input_ids, dtype=torch.long, device=self.device).unsqueeze(0)\n",
        "        input_len = (input_tensor != pad_idx).sum(dim=1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            enc_out, enc_hidden = self.encoder(input_tensor, input_len)\n",
        "\n",
        "            if self.arch == \"lstm\" and isinstance(enc_hidden, tuple):\n",
        "                h_0, c_0 = enc_hidden\n",
        "                h_0, c_0 = h_0.permute(1, 2, 0), c_0.permute(1, 2, 0)\n",
        "                h_0, c_0 = self.enc_dec(h_0), self.enc_dec(c_0)\n",
        "                hidden = (h_0.permute(2, 0, 1), c_0.permute(2, 0, 1))\n",
        "            else:\n",
        "                enc_hidden = enc_hidden.permute(1, 2, 0)\n",
        "                hidden = self.enc_dec(enc_hidden)\n",
        "                hidden = hidden.permute(2, 0, 1)\n",
        "\n",
        "            input_token = torch.full((1, 1), sos_idx, dtype=torch.long, device=self.device)\n",
        "            outputs = [input_token]\n",
        "\n",
        "            for _ in range(max_len - 1):\n",
        "                logits, hidden = self.decoder(input_token, hidden, enc_out)\n",
        "                next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
        "                outputs.append(next_token)\n",
        "                input_token = next_token\n",
        "\n",
        "                if next_token == eos_idx:\n",
        "                    break\n",
        "\n",
        "            # Concatenate predictions: [batch_size, seq_len]\n",
        "            generated = torch.cat(outputs, dim=1)\n",
        "\n",
        "            # Convert each sequence of token IDs to string\n",
        "            tokens = generated.tolist()\n",
        "            # Optionally truncate at eos token\n",
        "            if eos_idx in tokens:\n",
        "                tokens = tokens[:tokens.index(eos_idx)]\n",
        "            decoded = ''\n",
        "            clean_tokens = [token for token in tokens if token not in [pad_idx, sos_idx, eos_idx]]\n",
        "            for token in clean_tokens:\n",
        "                t = tokenizer.decode(token)\n",
        "                # print(t,end='')\n",
        "                decoded += t\n",
        "\n",
        "            return decoded\n"
      ],
      "metadata": {
        "id": "sE4DuVKmTEq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test"
      ],
      "metadata": {
        "id": "OtX8pGCAoRHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_seq2seq_forward_pass():\n",
        "    # Mock data\n",
        "    pairs = [\n",
        "        (\"hello world\", \"goodbye world\"),\n",
        "        (\"I am GPT\", \"hello world\"),\n",
        "        (\"i am fine\", \"this is good\"),\n",
        "        (\"nice to meet you\", \"hello world\"),\n",
        "    ]\n",
        "\n",
        "    tokenizer = get_tokenizer(path=\"/content/en_tokenizer.json\")\n",
        "    dataset = TranslationDataset(pairs, tokenizer)\n",
        "    dataloader = dataset.get_dataloader(batch_size=2)\n",
        "\n",
        "    # Model\n",
        "    vocab_size = len(tokenizer.get_vocab())\n",
        "    hidden_sz = 32\n",
        "    encoder = Encoder\n",
        "    decoder = Decoder\n",
        "    model = Seq2Seq(\n",
        "        encoder=encoder,\n",
        "        decoder=decoder,\n",
        "        vocab_size=vocab_size,\n",
        "        hidden_sz=hidden_sz,\n",
        "        enc_embed_sz=32,\n",
        "        dec_embed_sz=32,\n",
        "        arch=\"lstm\",\n",
        "        attn=\"dot\",\n",
        "        enc_bidir=True,\n",
        "        dec_bidir=True,\n",
        "        enc_num_layers=5,\n",
        "        dec_num_layers=2,\n",
        "        enc_dropout=0.1,\n",
        "        dec_dropout=0.1,\n",
        "    )\n",
        "\n",
        "    # Run forward pass on a batch\n",
        "    for src_batch, tgt_batch, src_lengths in dataloader:\n",
        "        print(\"Input shape:\", src_batch.shape)\n",
        "        print(\"Target shape:\", tgt_batch.shape)\n",
        "        output = model(src_batch, src_lengths, tgt_batch, tfr=0.5)\n",
        "        print(\"Output shape:\", output.shape)\n",
        "\n",
        "        assert output.shape[0] == tgt_batch.shape[0]       # batch size\n",
        "        assert output.shape[1] == tgt_batch.shape[1] - 1   # because predicting next token\n",
        "        assert output.shape[2] == vocab_size               # logits over vocab\n",
        "        print(\"✅ Test passed.\")\n",
        "        # break  # Only run one batch\n",
        "\n",
        "test_seq2seq_forward_pass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5u8Ud5VwoSWU",
        "outputId": "8e31a76c-f764-43bb-885e-b8204114ba1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 3])\n",
            "Target shape: torch.Size([2, 5])\n",
            "Output shape: torch.Size([2, 4, 8000])\n",
            "✅ Test passed.\n",
            "Input shape: torch.Size([2, 4])\n",
            "Target shape: torch.Size([2, 4])\n",
            "Output shape: torch.Size([2, 3, 8000])\n",
            "✅ Test passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "yfGJhwwZr_Uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, trainloader, optimizer, criterion, device, epochs=1, evaloader=None):\n",
        "    train_loss = 0\n",
        "    for e in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        model.train()\n",
        "        for src_tagged, tgt_tagged, src_len in tqdm.tqdm(trainloader, desc=\"Training\"):\n",
        "            src_padded = src_tagged.to(device)\n",
        "            tgt_padded = tgt_tagged.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src_padded, src_len, tgt_padded)\n",
        "            # compute the loss: compare 3D logits to 2D targets\n",
        "            loss = criterion(output.view(-1, output.shape[-1]), tgt_padded[:, 1:].reshape(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        if e < epochs-1:\n",
        "          print(f\"Epoch {e+1}/{epochs} completed with epoch loss of {epoch_loss/len(trainloader):.4f}\")\n",
        "        train_loss += epoch_loss/len(trainloader)\n",
        "        if (e+1)%5 != 0 or not evaloader:\n",
        "            continue\n",
        "        eval_loss = 0\n",
        "        model.eval()\n",
        "        for src_tagged, tgt_tagged, src_len in tqdm.tqdm(evaloader, desc=\"Validation\"):\n",
        "            src_padded = src_tagged.to(device)\n",
        "            tgt_padded = tgt_tagged.to(device)\n",
        "            with torch.no_grad():\n",
        "              output = model(src_padded, src_len, tgt_padded)\n",
        "              # compute the loss: compare 3D logits to 2D targets\n",
        "              loss = criterion(output.view(-1, output.shape[-1]), tgt_padded[:, 1:].reshape(-1))\n",
        "              eval_loss += loss.item()\n",
        "        print(f\"Evaluation completed with epoch loss of {eval_loss/len(evaloader):.4f}\")\n",
        "\n",
        "    print(f\"\\nTraining completed with training loss of {train_loss/epochs:.4f}\")"
      ],
      "metadata": {
        "id": "lgihFxc_sBj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing on dummy data for testing"
      ],
      "metadata": {
        "id": "vPYVbIklz7z3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Dummy data"
      ],
      "metadata": {
        "id": "giUe_ebH4_Ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eng_pairs = [\n",
        "    (\"How are you?\", \"How have you been?\"),\n",
        "    (\"I'm going to the store.\", \"I'm heading to the shop.\"),\n",
        "    (\"She likes reading books.\", \"She enjoys reading.\"),\n",
        "    (\"He is very tired.\", \"He's extremely exhausted.\"),\n",
        "    (\"Can you help me?\", \"Would you give me a hand?\"),\n",
        "    (\"It's raining outside.\", \"There is rain falling outside.\"),\n",
        "    (\"I love chocolate.\", \"Chocolate is my favorite.\"),\n",
        "    (\"What time is it?\", \"Do you know the time?\"),\n",
        "    (\"I'm learning Python.\", \"I'm studying Python.\"),\n",
        "    (\"That movie was great.\", \"I really liked that film.\"),\n",
        "    (\"Let's take a break.\", \"Let's pause for a moment.\"),\n",
        "    (\"The food was delicious.\", \"The meal tasted amazing.\"),\n",
        "    (\"He runs fast.\", \"He's a quick runner.\"),\n",
        "    (\"She's my best friend.\", \"She's my closest friend.\"),\n",
        "    (\"Do you speak English?\", \"Can you talk in English?\"),\n",
        "    (\"I need some water.\", \"I'm thirsty.\"),\n",
        "    (\"This book is interesting.\", \"This is a fascinating read.\"),\n",
        "    (\"Turn off the light.\", \"Switch the light off.\"),\n",
        "    (\"Please be quiet.\", \"Can you lower your voice?\"),\n",
        "    (\"I lost my keys.\", \"I can't find my keys.\"),\n",
        "    (\"Are you okay?\", \"Is everything alright?\"),\n",
        "    (\"I have no idea.\", \"I don't know.\"),\n",
        "    (\"It's too expensive.\", \"That costs a lot.\"),\n",
        "    (\"Where are you from?\", \"What is your hometown?\"),\n",
        "    (\"She sings beautifully.\", \"She has a lovely voice.\"),\n",
        "    (\"He's my brother.\", \"He's my sibling.\"),\n",
        "    (\"We are late.\", \"We’re behind schedule.\"),\n",
        "    (\"I'm going to bed.\", \"I'm off to sleep.\"),\n",
        "    (\"That's a good idea.\", \"That sounds great.\"),\n",
        "    (\"The weather is nice.\", \"It’s a beautiful day.\"),\n",
        "    (\"I'm sorry.\", \"I apologize.\"),\n",
        "    (\"I don't understand.\", \"I’m confused.\"),\n",
        "    (\"Could you repeat that?\", \"Can you say that again?\"),\n",
        "    (\"That was funny.\", \"I found it hilarious.\"),\n",
        "    (\"Do you want coffee?\", \"Would you like some coffee?\"),\n",
        "    (\"I forgot my password.\", \"I can't remember my password.\"),\n",
        "    (\"It's getting dark.\", \"The sun is going down.\"),\n",
        "    (\"Close the door.\", \"Shut the door.\"),\n",
        "    (\"She is very smart.\", \"She’s really intelligent.\"),\n",
        "    (\"I feel sick.\", \"I'm not feeling well.\"),\n",
        "    (\"We should go now.\", \"It's time to leave.\"),\n",
        "    (\"This is my house.\", \"I live here.\"),\n",
        "    (\"He lives nearby.\", \"He stays close.\"),\n",
        "    (\"What's your name?\", \"May I know your name?\"),\n",
        "    (\"Don't worry.\", \"It’s all right.\"),\n",
        "    (\"Can I ask you something?\", \"May I ask you a question?\"),\n",
        "    (\"I can't believe it!\", \"That's unbelievable!\"),\n",
        "    (\"They look the same.\", \"They appear identical.\"),\n",
        "    (\"I'm really busy.\", \"I have a lot to do.\"),\n",
        "    (\"He arrived late.\", \"He came after the scheduled time.\"),\n",
        "    (\"She is very kind.\", \"She’s really nice.\"),\n",
        "    (\"It’s not my fault.\", \"I’m not to blame.\"),\n",
        "    (\"Please sit down.\", \"Have a seat.\"),\n",
        "    (\"We’re having lunch.\", \"We’re eating now.\"),\n",
        "    (\"Let me see.\", \"Let me take a look.\"),\n",
        "    (\"I hate waiting.\", \"I dislike delays.\"),\n",
        "    (\"I'm on my way.\", \"I’m coming.\"),\n",
        "    (\"Don't be late.\", \"Be on time.\"),\n",
        "    (\"He made a mistake.\", \"He messed up.\"),\n",
        "    (\"She passed the exam.\", \"She succeeded in the test.\"),\n",
        "    (\"It’s broken.\", \"It doesn’t work.\"),\n",
        "    (\"I missed the bus.\", \"I didn’t catch the bus.\"),\n",
        "    (\"They are married.\", \"They’re husband and wife.\"),\n",
        "    (\"I’m feeling cold.\", \"It’s chilly.\"),\n",
        "    (\"She was surprised.\", \"She didn’t expect that.\"),\n",
        "    (\"I can't hear you.\", \"You're too quiet.\"),\n",
        "    (\"Let’s go shopping.\", \"Let’s buy some things.\"),\n",
        "    (\"He's a good driver.\", \"He drives well.\"),\n",
        "    (\"That’s not fair.\", \"That’s unjust.\"),\n",
        "    (\"I agree with you.\", \"You’re right.\"),\n",
        "    (\"Let’s start.\", \"Let’s begin.\"),\n",
        "    (\"I need a break.\", \"I want to rest.\"),\n",
        "    (\"This is confusing.\", \"I don’t get it.\"),\n",
        "    (\"She looks tired.\", \"She seems exhausted.\"),\n",
        "    (\"He’s always late.\", \"He never arrives on time.\"),\n",
        "    (\"I made it myself.\", \"I did it on my own.\"),\n",
        "    (\"Can you drive?\", \"Do you know how to drive?\"),\n",
        "    (\"It’s your turn.\", \"Now it’s up to you.\"),\n",
        "    (\"We had a great time.\", \"We really enjoyed ourselves.\"),\n",
        "    (\"Please call me.\", \"Give me a call.\"),\n",
        "    (\"That’s enough.\", \"Stop now.\"),\n",
        "    (\"I’m full.\", \"I can’t eat more.\"),\n",
        "    (\"Don’t forget.\", \"Remember that.\"),\n",
        "    (\"I was born here.\", \"This is my birthplace.\"),\n",
        "    (\"It’s near here.\", \"It’s close by.\"),\n",
        "    (\"You did well.\", \"Good job.\"),\n",
        "    (\"He’s rich.\", \"He has a lot of money.\"),\n",
        "    (\"This is fun.\", \"I’m enjoying this.\"),\n",
        "    (\"He’s very tall.\", \"He’s really big.\"),\n",
        "    (\"We’re done.\", \"We’ve finished.\"),\n",
        "    (\"Be careful.\", \"Watch out.\"),\n",
        "    (\"She’s shy.\", \"She’s introverted.\"),\n",
        "    (\"Try again.\", \"Give it another shot.\"),\n",
        "    (\"It’s not working.\", \"Something’s wrong.\"),\n",
        "    (\"I’ll be back soon.\", \"See you in a bit.\"),\n",
        "    (\"She won the prize.\", \"She got the award.\"),\n",
        "    (\"It’s your choice.\", \"You decide.\"),\n",
        "    (\"Keep going.\", \"Continue.\"),\n",
        "    (\"I’m listening.\", \"Go ahead, I’m paying attention.\")\n",
        "]"
      ],
      "metadata": {
        "id": "-ME2f6IDz-zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Trainig Loop"
      ],
      "metadata": {
        "id": "li7TgyES5Du5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer(path=\"/content/en_tokenizer.json\")\n",
        "dataset = TranslationDataset(eng_pairs, tokenizer)\n",
        "dataloader = dataset.get_dataloader(batch_size=2)\n",
        "encoder = Encoder\n",
        "decoder = Decoder\n",
        "model = Seq2Seq(\n",
        "    encoder=encoder,\n",
        "    decoder=decoder,\n",
        "    vocab_size=len(tokenizer.get_vocab()),\n",
        "    hidden_sz=32,\n",
        "    enc_num_layers=2,\n",
        "    dec_num_layers=2,\n",
        "    arch=\"lstm\",\n",
        "    attn=\"dot\"\n",
        ")\n",
        "\n",
        "train(model, dataloader, optimizer=torch.optim.Adam(model.parameters()), criterion=nn.CrossEntropyLoss(), device=\"cpu\", epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbjV23oQ0Qeq",
        "outputId": "36c88f3f-a30d-4a85-9fb3-b28656a26d54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  3.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 completed with epoch loss of 6.8778\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  3.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100 completed with epoch loss of 4.1669\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  3.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/100 completed with epoch loss of 3.6537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  4.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/100 completed with epoch loss of 3.3617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  4.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/100 completed with epoch loss of 3.0598\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/100 completed with epoch loss of 2.8040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/100 completed with epoch loss of 2.5444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/100 completed with epoch loss of 2.2566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  4.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/100 completed with epoch loss of 2.0064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100 completed with epoch loss of 1.8166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/100 completed with epoch loss of 1.6023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/100 completed with epoch loss of 1.4304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/100 completed with epoch loss of 1.1593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:10<00:00,  4.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/100 completed with epoch loss of 0.9467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/100 completed with epoch loss of 0.7584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  4.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/100 completed with epoch loss of 0.6206\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  3.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/100 completed with epoch loss of 0.5369\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  3.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/100 completed with epoch loss of 0.5097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/100 completed with epoch loss of 0.4754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/100 completed with epoch loss of 0.3521\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  3.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/100 completed with epoch loss of 0.2532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  3.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/100 completed with epoch loss of 0.1811\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:13<00:00,  3.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/100 completed with epoch loss of 0.1416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:13<00:00,  3.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/100 completed with epoch loss of 0.1263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:13<00:00,  3.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/100 completed with epoch loss of 0.0993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:13<00:00,  3.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/100 completed with epoch loss of 0.0775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/100 completed with epoch loss of 0.0719\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:13<00:00,  3.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/100 completed with epoch loss of 0.0619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/100 completed with epoch loss of 0.0494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  3.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/100 completed with epoch loss of 0.0406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/100 completed with epoch loss of 0.0354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  3.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/100 completed with epoch loss of 0.0315\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/100 completed with epoch loss of 0.0264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:10<00:00,  4.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/100 completed with epoch loss of 0.0232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/100 completed with epoch loss of 0.0204\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36/100 completed with epoch loss of 0.0183\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  4.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37/100 completed with epoch loss of 0.0169\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  3.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38/100 completed with epoch loss of 0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:10<00:00,  4.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39/100 completed with epoch loss of 0.0142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  4.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/100 completed with epoch loss of 0.0129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  3.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41/100 completed with epoch loss of 0.0120\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  4.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42/100 completed with epoch loss of 0.0112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43/100 completed with epoch loss of 0.0104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44/100 completed with epoch loss of 0.0096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  3.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45/100 completed with epoch loss of 0.0089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  3.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46/100 completed with epoch loss of 0.0084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47/100 completed with epoch loss of 0.0077\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48/100 completed with epoch loss of 0.0073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  4.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49/100 completed with epoch loss of 0.0070\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/100 completed with epoch loss of 0.0064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  4.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 51/100 completed with epoch loss of 0.0061\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:13<00:00,  3.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 52/100 completed with epoch loss of 0.0057\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  3.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 53/100 completed with epoch loss of 0.0054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 54/100 completed with epoch loss of 0.0051\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 55/100 completed with epoch loss of 0.0050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  3.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 56/100 completed with epoch loss of 0.0046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:13<00:00,  3.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 57/100 completed with epoch loss of 0.0044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  4.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 58/100 completed with epoch loss of 0.0041\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:13<00:00,  3.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 59/100 completed with epoch loss of 0.0039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  4.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 60/100 completed with epoch loss of 0.0038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  3.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 61/100 completed with epoch loss of 0.0035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  4.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 62/100 completed with epoch loss of 0.0034\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 63/100 completed with epoch loss of 0.0032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 64/100 completed with epoch loss of 0.0031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  4.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 65/100 completed with epoch loss of 0.0029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  3.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 66/100 completed with epoch loss of 0.0029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 67/100 completed with epoch loss of 0.0027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  3.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 68/100 completed with epoch loss of 0.0026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  4.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 69/100 completed with epoch loss of 0.0024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:13<00:00,  3.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 70/100 completed with epoch loss of 0.0023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  4.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 71/100 completed with epoch loss of 0.0022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 72/100 completed with epoch loss of 0.0021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:10<00:00,  4.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 73/100 completed with epoch loss of 0.0020\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:13<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 74/100 completed with epoch loss of 0.0019\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:13<00:00,  3.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 75/100 completed with epoch loss of 0.0019\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  3.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 76/100 completed with epoch loss of 0.0018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  3.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 77/100 completed with epoch loss of 0.0017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  3.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 78/100 completed with epoch loss of 0.0016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  4.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 79/100 completed with epoch loss of 0.0016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 80/100 completed with epoch loss of 0.0015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 81/100 completed with epoch loss of 0.0015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:13<00:00,  3.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 82/100 completed with epoch loss of 0.0014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:13<00:00,  3.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 83/100 completed with epoch loss of 0.0013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 84/100 completed with epoch loss of 0.0013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:13<00:00,  3.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 85/100 completed with epoch loss of 0.0012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  3.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 86/100 completed with epoch loss of 0.0012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  3.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 87/100 completed with epoch loss of 0.0011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  3.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 88/100 completed with epoch loss of 0.0011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:13<00:00,  3.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 89/100 completed with epoch loss of 0.0011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 90/100 completed with epoch loss of 0.0010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  3.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 91/100 completed with epoch loss of 0.0010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 92/100 completed with epoch loss of 0.0010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 93/100 completed with epoch loss of 0.0009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 94/100 completed with epoch loss of 0.0009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:12<00:00,  3.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 95/100 completed with epoch loss of 0.0008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 96/100 completed with epoch loss of 0.0008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:13<00:00,  3.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 97/100 completed with epoch loss of 0.0008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:13<00:00,  3.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 98/100 completed with epoch loss of 0.0008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:11<00:00,  4.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 99/100 completed with epoch loss of 0.0007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:13<00:00,  3.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training completed with training loss of 0.4246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicting"
      ],
      "metadata": {
        "id": "2GfxcNE743BV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, tokenizer, input_seq):\n",
        "    output = model.generate(input_seq, tokenizer)\n",
        "\n",
        "for src, tgt in eng_pairs[:4]:\n",
        "    print(f\"Source: {src}\")\n",
        "    print(f\"Target: {tgt}\")\n",
        "    print(\"Predicted: \")\n",
        "    predict(model, tokenizer, src)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVoW8Zmt45IP",
        "outputId": "84894233-0d02-4ca3-99f6-edcdb5972d10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source: How are you?\n",
            "Target: How have you been?\n",
            "Predicted: \n",
            " ow have you been?\n",
            "Source: I'm going to the store.\n",
            "Target: I'm heading to the shop.\n",
            "Predicted: \n",
            " 'm heading to the shop.\n",
            "Source: She likes reading books.\n",
            "Target: She enjoys reading.\n",
            "Predicted: \n",
            " he enjoys reading.\n",
            "Source: He is very tired.\n",
            "Target: He's extremely exhausted.\n",
            "Predicted: \n",
            " e's extremely exhausted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# extra"
      ],
      "metadata": {
        "id": "nJ2OuAOtdJes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'model.pth')"
      ],
      "metadata": {
        "id": "DU8EUIkwE5PE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jhi7xEw7dH5E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}